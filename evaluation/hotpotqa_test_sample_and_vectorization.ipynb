{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d3b317f",
   "metadata": {},
   "source": [
    "In this notebook:\n",
    "- Generates a sample from hotpotqa dataset, load its documents(sentences) to vector db, and saves the sample as a json for future use.\n",
    "- The sentences loaded as they are, no further chunking was done due to HotPotQA's nature.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852c2f7b",
   "metadata": {},
   "source": [
    "### Sample HotpotQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad09f4d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'question', 'answer', 'type', 'level', 'supporting_facts', 'context'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_dataset(\"hotpotqa/hotpot_qa\", \"distractor\")\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "# Select a random subset of 1000 samples\n",
    "shuffled_dataset = train_dataset.shuffle(seed=42)\n",
    "random_sample = shuffled_dataset.select(range(1000))\n",
    "\n",
    "# Save to a local file for reference\n",
    "random_sample.to_json(\"hotpotqa_1000_samples.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f81186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare documents to load\n",
    "documents_to_load = []\n",
    "seen_titles = set()\n",
    "\n",
    "for row in random_sample:\n",
    "    titles = row['context']['title']\n",
    "    paragraphs = row['context']['sentences']\n",
    "\n",
    "    for i in range(len(titles)):\n",
    "        title = titles[i]\n",
    "        \n",
    "        # Only add if we haven't seen this specific Wikipedia page yet\n",
    "        if title not in seen_titles:\n",
    "            # Join the list of sentences into one single string (paragraph)\n",
    "            full_text = \" \".join(paragraphs[i])\n",
    "            documents_to_load.append(full_text)\n",
    "            \n",
    "            seen_titles.add(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3aa058",
   "metadata": {},
   "source": [
    "### Load to Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a85bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# to import from parent directory\n",
    "parent_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if parent_path not in sys.path:\n",
    "    sys.path.append(parent_path)\n",
    "\n",
    "# import vector db client\n",
    "from vector_db.src.client import VectorDBClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00c9609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate vector db client\n",
    "vector_db_client = VectorDBClient(persist_directory=\"../vector_db/chroma_db\")\n",
    "\n",
    "# empty vector db\n",
    "vector_db_client.delete_collection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9577c615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding 1000 whole documents...\n",
      "Storing in DB...\n",
      "Embedding 1000 whole documents...\n",
      "Storing in DB...\n",
      "Embedding 1000 whole documents...\n",
      "Storing in DB...\n",
      "Embedding 1000 whole documents...\n",
      "Storing in DB...\n",
      "Embedding 1000 whole documents...\n",
      "Storing in DB...\n",
      "Embedding 1000 whole documents...\n",
      "Storing in DB...\n",
      "Embedding 1000 whole documents...\n",
      "Storing in DB...\n",
      "Embedding 1000 whole documents...\n",
      "Storing in DB...\n",
      "Embedding 1000 whole documents...\n",
      "Storing in DB...\n",
      "Embedding 811 whole documents...\n",
      "Storing in DB...\n"
     ]
    }
   ],
   "source": [
    "# batch load the documents_to_load as chunks of 1000 to the vector db\n",
    "for i in range(0, len(documents_to_load), 1000):\n",
    "    batch = documents_to_load[i:i+1000]\n",
    "    vector_db_client.add_documents_no_chunking(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d4515a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
