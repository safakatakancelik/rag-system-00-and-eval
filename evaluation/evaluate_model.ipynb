{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a79ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be9d79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from ragas.testset import TestsetGenerator\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e09fdcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5bd1fd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for file in os.listdir(\"./\"):\n",
    "    if file.endswith(\"test.pdf\"):\n",
    "        loader = PyPDFLoader(f\"./{file}\")\n",
    "        docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de8a8fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "judge_llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ac33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# 1. Initialize LangChain objects\n",
    "lang_llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "lang_embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 2. Wrap them for Ragas compatibility\n",
    "# This adds the .embed_text() method that Ragas is looking for\n",
    "generator_llm = LangchainLLMWrapper(lang_llm)\n",
    "generator_embeddings = LangchainEmbeddingsWrapper(lang_embeddings)\n",
    "\n",
    "# 3. Initialize the generator with the WRAPPED versions\n",
    "from ragas.testset import TestsetGenerator\n",
    "generator = TestsetGenerator(\n",
    "    llm=generator_llm, \n",
    "    embedding_model=generator_embeddings\n",
    ")\n",
    "\n",
    "# 4. If you are using custom transforms, they must also use the wrapped embeddings\n",
    "from ragas.testset.transforms import default_transforms\n",
    "transforms = default_transforms(documents=docs, llm=generator_llm, embedding_model=generator_embeddings)\n",
    "\n",
    "# 5. Run\n",
    "test_set = generator.generate_with_langchain_docs(docs, testset_size=2, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6578b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_df = test_set.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4359da4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>reference_contexts</th>\n",
       "      <th>reference</th>\n",
       "      <th>persona_name</th>\n",
       "      <th>query_style</th>\n",
       "      <th>query_length</th>\n",
       "      <th>synthesizer_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wht role did Safak Atakan Celik play in Street...</td>\n",
       "      <td>[Safak Atakan Celik\\n(data analyst)\\n (778) 95...</td>\n",
       "      <td>Safak Atakan Celik served as the Data Processi...</td>\n",
       "      <td>Data Analytics Professional</td>\n",
       "      <td>MISSPELLED</td>\n",
       "      <td>LONG</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What kind of solutions did Streetbees provide ...</td>\n",
       "      <td>[Safak Atakan Celik\\n(data analyst)\\n (778) 95...</td>\n",
       "      <td>Streetbees delivered end-to-end solutions that...</td>\n",
       "      <td>Data Analytics Professional</td>\n",
       "      <td>WEB_SEARCH_LIKE</td>\n",
       "      <td>MEDIUM</td>\n",
       "      <td>single_hop_specific_query_synthesizer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  Wht role did Safak Atakan Celik play in Street...   \n",
       "1  What kind of solutions did Streetbees provide ...   \n",
       "\n",
       "                                  reference_contexts  \\\n",
       "0  [Safak Atakan Celik\\n(data analyst)\\n (778) 95...   \n",
       "1  [Safak Atakan Celik\\n(data analyst)\\n (778) 95...   \n",
       "\n",
       "                                           reference  \\\n",
       "0  Safak Atakan Celik served as the Data Processi...   \n",
       "1  Streetbees delivered end-to-end solutions that...   \n",
       "\n",
       "                  persona_name      query_style query_length  \\\n",
       "0  Data Analytics Professional       MISSPELLED         LONG   \n",
       "1  Data Analytics Professional  WEB_SEARCH_LIKE       MEDIUM   \n",
       "\n",
       "                        synthesizer_name  \n",
       "0  single_hop_specific_query_synthesizer  \n",
       "1  single_hop_specific_query_synthesizer  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ef6a7fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Your RAG system logic (simulated here)\n",
    "# Important: ensure you process the questions in the EXACT order of the test_set_df\n",
    "rag_answers = [\"Paris is the capital.\", \"Transformers work in parallel.\"]\n",
    "rag_contexts = [\n",
    "    [\"Paris is the capital of France.\"], \n",
    "    [\"Transformers use self-attention to process data in parallel.\"]\n",
    "]\n",
    "\n",
    "# 2. Build the combined DataFrame\n",
    "eval_df = pd.DataFrame({\n",
    "    \"question\": test_set_df[\"user_input\"].tolist(),\n",
    "    \"ground_truth\": test_set_df[\"reference\"].tolist(),\n",
    "    \"answer\": rag_answers,\n",
    "    \"contexts\": rag_contexts\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "486eb782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>answer</th>\n",
       "      <th>contexts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wht role did Safak Atakan Celik play in Street...</td>\n",
       "      <td>Safak Atakan Celik served as the Data Processi...</td>\n",
       "      <td>Paris is the capital.</td>\n",
       "      <td>[Paris is the capital of France.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What kind of solutions did Streetbees provide ...</td>\n",
       "      <td>Streetbees delivered end-to-end solutions that...</td>\n",
       "      <td>Transformers work in parallel.</td>\n",
       "      <td>[Transformers use self-attention to process da...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Wht role did Safak Atakan Celik play in Street...   \n",
       "1  What kind of solutions did Streetbees provide ...   \n",
       "\n",
       "                                        ground_truth  \\\n",
       "0  Safak Atakan Celik served as the Data Processi...   \n",
       "1  Streetbees delivered end-to-end solutions that...   \n",
       "\n",
       "                           answer  \\\n",
       "0           Paris is the capital.   \n",
       "1  Transformers work in parallel.   \n",
       "\n",
       "                                            contexts  \n",
       "0                  [Paris is the capital of France.]  \n",
       "1  [Transformers use self-attention to process da...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca3ec05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d857a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f6646c6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "ollama\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0569240",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas.llms import LangchainLLMWrapper\n",
    "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
    "from langchain_ollama import ChatOllama, OllamaEmbeddings\n",
    "from ragas.testset import TestsetGenerator\n",
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba0d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize the Qwen LLM and Embeddings from Ollama\n",
    "# Qwen 2.5 7B is highly recommended for this\n",
    "local_llm = ChatOllama(model=\"qwen2.5:7b\")\n",
    "local_embeddings = OllamaEmbeddings(model=\"nomic-embed-text\") # Or \"qwen2.5:7b\" if preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be015ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\safak\\AppData\\Local\\Temp\\ipykernel_13600\\2254106299.py:2: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  generator_llm = LangchainLLMWrapper(local_llm)\n",
      "C:\\Users\\safak\\AppData\\Local\\Temp\\ipykernel_13600\\2254106299.py:3: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use llm_factory instead: from openai import OpenAI; from ragas.llms import llm_factory; llm = llm_factory('gpt-4o-mini', client=OpenAI(api_key='...'))\n",
      "  critic_llm = LangchainLLMWrapper(local_llm)\n",
      "C:\\Users\\safak\\AppData\\Local\\Temp\\ipykernel_13600\\2254106299.py:4: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
      "  embeddings = LangchainEmbeddingsWrapper(local_embeddings)\n"
     ]
    }
   ],
   "source": [
    "# 2. Wrap them for Ragas compatibility\n",
    "generator_llm = LangchainLLMWrapper(local_llm)\n",
    "critic_llm = LangchainLLMWrapper(local_llm)\n",
    "embeddings = LangchainEmbeddingsWrapper(local_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f00f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize the Generator\n",
    "generator = TestsetGenerator(\n",
    "    generator_llm,\n",
    "    critic_llm,\n",
    "    embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ce8e899",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for file in os.listdir(\"./\"):\n",
    "    if file.endswith(\"test.pdf\"):\n",
    "        loader = PyPDFLoader(f\"./{file}\")\n",
    "        docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "562f7858",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Applying HeadlinesExtractor:   0%|          | 0/1 [00:00<?, ?it/s]Task failed with ResponseError: model requires more system memory (5.6 GiB) than is available (4.5 GiB) (status code: 500)\n",
      "Applying HeadlinesExtractor: 100%|██████████| 1/1 [00:39<00:00, 39.18s/it]\n"
     ]
    },
    {
     "ename": "ResponseError",
     "evalue": "model requires more system memory (5.6 GiB) than is available (4.5 GiB) (status code: 500)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mResponseError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mragas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrun_config\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RunConfig\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# 4. Generate the testset (assuming 'docs' is your Langchain document list)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m dataset = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestset_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRunConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\testset\\synthesizers\\generate.py:203\u001b[39m, in \u001b[36mTestsetGenerator.generate_with_langchain_docs\u001b[39m\u001b[34m(self, documents, testset_size, transforms, transforms_llm, transforms_embedding_model, query_distribution, run_config, callbacks, token_usage_parser, with_debugging_logs, raise_exceptions, return_executor)\u001b[39m\n\u001b[32m    200\u001b[39m kg = KnowledgeGraph(nodes=nodes)\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# apply transforms and update the knowledge graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[43mapply_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mRunConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28mself\u001b[39m.knowledge_graph = kg\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate(\n\u001b[32m    207\u001b[39m     testset_size=testset_size,\n\u001b[32m    208\u001b[39m     query_distribution=query_distribution,\n\u001b[32m   (...)\u001b[39m\u001b[32m    214\u001b[39m     return_executor=return_executor,\n\u001b[32m    215\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\testset\\transforms\\engine.py:70\u001b[39m, in \u001b[36mapply_transforms\u001b[39m\u001b[34m(kg, transforms, run_config, callbacks)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transforms, t.Sequence):\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m transforms:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m         \u001b[43mapply_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transforms, Parallel):\n\u001b[32m     72\u001b[39m     apply_transforms(kg, transforms.transformations, run_config, callbacks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\testset\\transforms\\engine.py:79\u001b[39m, in \u001b[36mapply_transforms\u001b[39m\u001b[34m(kg, transforms, run_config, callbacks)\u001b[39m\n\u001b[32m     77\u001b[39m     coros = transforms.generate_execution_plan(kg)\n\u001b[32m     78\u001b[39m     desc = get_desc(transforms)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     \u001b[43mrun_async_tasks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcoros\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar_desc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     88\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid transforms type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(transforms)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Expects a sequence of BaseGraphTransformations or a Parallel instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\async_utils.py:232\u001b[39m, in \u001b[36mrun_async_tasks\u001b[39m\u001b[34m(tasks, batch_size, show_progress, progress_bar_desc, max_workers, cancel_check)\u001b[39m\n\u001b[32m    228\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m first_exception\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\async_utils.py:156\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(async_func, allow_nest_asyncio)\u001b[39m\n\u001b[32m    148\u001b[39m     loop_type = \u001b[38;5;28mtype\u001b[39m(loop).\u001b[34m__name__\u001b[39m\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    150\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot execute nested async code with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloop_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    151\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33muvloop does not support nested event loop execution. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    152\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPlease use asyncio\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms standard event loop in Jupyter environments, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    153\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mor refactor your code to avoid nested async calls.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    154\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoro\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\async_utils.py:228\u001b[39m, in \u001b[36mrun_async_tasks.<locals>._run\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    226\u001b[39m \u001b[38;5;66;03m# Raise the first exception encountered to fail fast with clear error message\u001b[39;00m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m first_exception\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\async_utils.py:114\u001b[39m, in \u001b[36mprocess_futures\u001b[39m\u001b[34m(futures)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m futures:\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m asyncio.CancelledError:\n\u001b[32m    116\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# Re-raise CancelledError to ensure proper cancellation\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:634\u001b[39m, in \u001b[36m_AsCompletedIterator._wait_for_one\u001b[39m\u001b[34m(self, resolve)\u001b[39m\n\u001b[32m    631\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    632\u001b[39m     \u001b[38;5;66;03m# Dummy value from _handle_timeout().\u001b[39;00m\n\u001b[32m    633\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.TimeoutError\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m resolve \u001b[38;5;28;01melse\u001b[39;00m f\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\futures.py:199\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\async_utils.py:77\u001b[39m, in \u001b[36mas_completed.<locals>.sema_coro\u001b[39m\u001b[34m(coro)\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msema_coro\u001b[39m(coro):\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\testset\\transforms\\base.py:202\u001b[39m, in \u001b[36mExtractor.generate_execution_plan.<locals>.apply_extract\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_extract\u001b[39m(node: Node):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     property_name, property_value = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.extract(node)\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m node.get_property(property_name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    204\u001b[39m         node.add_property(property_name, property_value)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\testset\\transforms\\extractors\\llm_based.py:264\u001b[39m, in \u001b[36mHeadlinesExtractor.extract\u001b[39m\u001b[34m(self, node)\u001b[39m\n\u001b[32m    262\u001b[39m headlines = []\n\u001b[32m    263\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.prompt.generate(\n\u001b[32m    265\u001b[39m         \u001b[38;5;28mself\u001b[39m.llm, data=TextWithExtractionLimit(text=chunk, max_num=\u001b[38;5;28mself\u001b[39m.max_num)\n\u001b[32m    266\u001b[39m     )\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m    268\u001b[39m         headlines.extend(result.headlines)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:177\u001b[39m, in \u001b[36mPydanticPrompt.generate\u001b[39m\u001b[34m(self, llm, data, temperature, stop, callbacks, retries_left)\u001b[39m\n\u001b[32m    174\u001b[39m callbacks = callbacks \u001b[38;5;129;01mor\u001b[39;00m []\n\u001b[32m    176\u001b[39m \u001b[38;5;66;03m# this is just a special case of generate_multiple\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m output_single = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate_multiple(\n\u001b[32m    178\u001b[39m     llm=llm,\n\u001b[32m    179\u001b[39m     data=data,\n\u001b[32m    180\u001b[39m     n=\u001b[32m1\u001b[39m,\n\u001b[32m    181\u001b[39m     temperature=temperature,\n\u001b[32m    182\u001b[39m     stop=stop,\n\u001b[32m    183\u001b[39m     callbacks=callbacks,\n\u001b[32m    184\u001b[39m     retries_left=retries_left,\n\u001b[32m    185\u001b[39m )\n\u001b[32m    186\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output_single[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\prompt\\pydantic_prompt.py:277\u001b[39m, in \u001b[36mPydanticPrompt.generate_multiple\u001b[39m\u001b[34m(self, llm, data, n, temperature, stop, callbacks, retries_left)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    275\u001b[39m     \u001b[38;5;66;03m# This is a standard BaseRagasLLM - use generate()\u001b[39;00m\n\u001b[32m    276\u001b[39m     ragas_llm = t.cast(BaseRagasLLM, llm)\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m ragas_llm.generate(\n\u001b[32m    278\u001b[39m         prompt_value,\n\u001b[32m    279\u001b[39m         n=n,\n\u001b[32m    280\u001b[39m         temperature=temperature,\n\u001b[32m    281\u001b[39m         stop=stop,\n\u001b[32m    282\u001b[39m         callbacks=prompt_cb,\n\u001b[32m    283\u001b[39m     )\n\u001b[32m    285\u001b[39m output_models = []\n\u001b[32m    286\u001b[39m parser = RagasOutputParser(pydantic_object=\u001b[38;5;28mself\u001b[39m.output_model)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\llms\\base.py:116\u001b[39m, in \u001b[36mBaseRagasLLM.generate\u001b[39m\u001b[34m(self, prompt, n, temperature, stop, callbacks)\u001b[39m\n\u001b[32m    111\u001b[39m     temperature = \u001b[38;5;28mself\u001b[39m.get_temperature(n)\n\u001b[32m    113\u001b[39m agenerate_text_with_retry = add_async_retry(\n\u001b[32m    114\u001b[39m     \u001b[38;5;28mself\u001b[39m.agenerate_text, \u001b[38;5;28mself\u001b[39m.run_config\n\u001b[32m    115\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m agenerate_text_with_retry(\n\u001b[32m    117\u001b[39m     prompt=prompt,\n\u001b[32m    118\u001b[39m     n=n,\n\u001b[32m    119\u001b[39m     temperature=temperature,\n\u001b[32m    120\u001b[39m     stop=stop,\n\u001b[32m    121\u001b[39m     callbacks=callbacks,\n\u001b[32m    122\u001b[39m )\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# check there are no max_token issues\u001b[39;00m\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_finished(result):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189\u001b[39m, in \u001b[36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    188\u001b[39m async_wrapped.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\tenacity\\_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\tenacity\\__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\tenacity\\__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    116\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ragas\\llms\\base.py:294\u001b[39m, in \u001b[36mLangchainLLMWrapper.agenerate_text\u001b[39m\u001b[34m(self, prompt, n, temperature, stop, callbacks)\u001b[39m\n\u001b[32m    288\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.langchain_llm.agenerate_prompt(\n\u001b[32m    289\u001b[39m         prompts=[prompt],\n\u001b[32m    290\u001b[39m         stop=stop,\n\u001b[32m    291\u001b[39m         callbacks=callbacks,\n\u001b[32m    292\u001b[39m     )\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.langchain_llm.agenerate_prompt(\n\u001b[32m    295\u001b[39m         prompts=[prompt] * n,\n\u001b[32m    296\u001b[39m         stop=stop,\n\u001b[32m    297\u001b[39m         callbacks=callbacks,\n\u001b[32m    298\u001b[39m     )\n\u001b[32m    299\u001b[39m     \u001b[38;5;66;03m# make LLMResult.generation appear as if it was n_completions\u001b[39;00m\n\u001b[32m    300\u001b[39m     \u001b[38;5;66;03m# note that LLMResult.runs is still a list that represents each run\u001b[39;00m\n\u001b[32m    301\u001b[39m     generations = [[g[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m result.generations]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1128\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1119\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m   1121\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1125\u001b[39m     **kwargs: Any,\n\u001b[32m   1126\u001b[39m ) -> LLMResult:\n\u001b[32m   1127\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m   1129\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m   1130\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1086\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m   1073\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[32m   1074\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m   1075\u001b[39m             *[\n\u001b[32m   1076\u001b[39m                 run_manager.on_llm_end(\n\u001b[32m   (...)\u001b[39m\u001b[32m   1084\u001b[39m             ]\n\u001b[32m   1085\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1086\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[32m0\u001b[39m]\n\u001b[32m   1087\u001b[39m flattened_outputs = [\n\u001b[32m   1088\u001b[39m     LLMResult(generations=[res.generations], llm_output=res.llm_output)  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m   1089\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[32m   1090\u001b[39m ]\n\u001b[32m   1091\u001b[39m llm_output = \u001b[38;5;28mself\u001b[39m._combine_llm_outputs([res.llm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\tasks.py:304\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    301\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    302\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    303\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m304\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    306\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1339\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1337\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1338\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1339\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1340\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1341\u001b[39m     )\n\u001b[32m   1342\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1343\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:1208\u001b[39m, in \u001b[36mChatOllama._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1201\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_agenerate\u001b[39m(\n\u001b[32m   1202\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1203\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1206\u001b[39m     **kwargs: Any,\n\u001b[32m   1207\u001b[39m ) -> ChatResult:\n\u001b[32m-> \u001b[39m\u001b[32m1208\u001b[39m     final_chunk = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._achat_stream_with_aggregation(\n\u001b[32m   1209\u001b[39m         messages, stop, run_manager, verbose=\u001b[38;5;28mself\u001b[39m.verbose, **kwargs\n\u001b[32m   1210\u001b[39m     )\n\u001b[32m   1211\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m   1212\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m   1213\u001b[39m         message=AIMessage(\n\u001b[32m   1214\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1221\u001b[39m         generation_info=generation_info,\n\u001b[32m   1222\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:991\u001b[39m, in \u001b[36mChatOllama._achat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    982\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_achat_stream_with_aggregation\u001b[39m(\n\u001b[32m    983\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    984\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    988\u001b[39m     **kwargs: Any,\n\u001b[32m    989\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    990\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m991\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aiterate_over_stream(messages, stop, **kwargs):\n\u001b[32m    992\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m final_chunk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    993\u001b[39m             final_chunk = chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:1131\u001b[39m, in \u001b[36mChatOllama._aiterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m   1124\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_aiterate_over_stream\u001b[39m(\n\u001b[32m   1125\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1126\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   1127\u001b[39m     stop: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1128\u001b[39m     **kwargs: Any,\n\u001b[32m   1129\u001b[39m ) -> AsyncIterator[ChatGenerationChunk]:\n\u001b[32m   1130\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acreate_chat_stream(messages, stop, **kwargs):\n\u001b[32m   1132\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   1133\u001b[39m             content = (\n\u001b[32m   1134\u001b[39m                 stream_resp[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1135\u001b[39m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1136\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1137\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\langchain_ollama\\chat_models.py:937\u001b[39m, in \u001b[36mChatOllama._acreate_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    934\u001b[39m chat_params = \u001b[38;5;28mself\u001b[39m._chat_params(messages, stop, **kwargs)\n\u001b[32m    936\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m937\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_client.chat(**chat_params):\n\u001b[32m    938\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\safak\\Desktop\\devs\\git_repos\\search-systems-self\\my_venv\\Lib\\site-packages\\ollama\\_client.py:757\u001b[39m, in \u001b[36mAsyncClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    756\u001b[39m   \u001b[38;5;28;01mawait\u001b[39;00m e.response.aread()\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.aiter_lines():\n\u001b[32m    760\u001b[39m   part = json.loads(line)\n",
      "\u001b[31mResponseError\u001b[39m: model requires more system memory (5.6 GiB) than is available (4.5 GiB) (status code: 500)"
     ]
    }
   ],
   "source": [
    "from ragas.run_config import RunConfig\n",
    "# 4. Generate the testset (assuming 'docs' is your Langchain document list)\n",
    "dataset = generator.generate_with_langchain_docs(docs, testset_size=2, run_config=RunConfig(max_workers=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec476a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
